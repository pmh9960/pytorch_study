{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "ps_36",
   "display_name": "ps 3.6 Kernel",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Quantization\n",
    "\n",
    "## WHY?\n",
    "\n",
    "float32의 연산이 int보다 복잡하다.  \n",
    "float32의 구조가 부동소수점 구조인데  \n",
    "\n",
    "| sign | exponential | mantissa |\n",
    "| :--: | :---------: | :------: |\n",
    "|  1   |      8      |    32    |\n",
    "\n",
    "연산이 int보다 같은 bit에서 대략 4배 느리다.  \n",
    "수를 int 구조로 바꿔서 연산하는 방식을 채택.\n",
    "\n",
    "근거 중 하나는 부동소수점 구조가 넓은 범위의 수를 표현하기 위해서 만들어져있는데 weight 값들이 생각보다 넓게 퍼져있지 않음.  \n",
    "이를 이용하여 (min, max) or (mean, scale)의 값을 이용하여 균일한 간격으로 분포시킬 수 있음.\n",
    "\n",
    "## In pytorch\n",
    "\n",
    "자체 함수가 존재. 하지만 아직 cpu에서 연산한다. (원래 gpu에서도 가능은 함)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[ 0.1803, -1.7519, -0.2755],\n",
       "         [ 1.0332, -0.0269, -1.3415]],\n",
       "\n",
       "        [[ 0.5794, -0.2408,  1.4850],\n",
       "         [-2.3509,  1.7678, -0.7412]]])"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "float_tensor = torch.randn(2,2,3)\n",
    "float_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[ 0.1800, -1.2800, -0.2800],\n",
       "         [ 1.0300, -0.0300, -1.2800]],\n",
       "\n",
       "        [[ 0.5800, -0.2400,  1.2700],\n",
       "         [-1.2800,  1.2700, -0.7400]]], size=(2, 2, 3), dtype=torch.qint8,\n",
       "       quantization_scheme=torch.per_tensor_affine, scale=0.01, zero_point=0)"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "zero_point = 0 # 반드시 int 여야 함.\n",
    "scale = 1/100\n",
    "q_tensor = torch.quantize_per_tensor(float_tensor, scale=scale, zero_point=zero_point, dtype=torch.qint8)\n",
    "q_tensor"
   ]
  },
  {
   "source": [
    "위 결과를 보면 scale = 1/100, zero_point = 0에서 가능한 값의 범위는 \\[-1.28, 1.27\\]이다.  \n",
    "그 밖의 결과는 clip되는 효과.\n",
    "\n",
    "만약 범위를 늘리고 싶다면 scale을 키우거나 qint bit 수를 키우면 된다. (e.g. scale = 1/50)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[ 0.1800, -1.7600, -0.2800],\n",
       "         [ 1.0400, -0.0200, -1.3400]],\n",
       "\n",
       "        [[ 0.5800, -0.2400,  1.4800],\n",
       "         [-2.3600,  1.7600, -0.7400]]], size=(2, 2, 3), dtype=torch.qint8,\n",
       "       quantization_scheme=torch.per_tensor_affine, scale=0.02, zero_point=0)"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "scale = 1/50\n",
    "q_tensor_1 = torch.quantize_per_tensor(float_tensor, scale=scale, zero_point=zero_point, dtype=torch.qint8)\n",
    "q_tensor_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[ 0.2000, -1.2800, -0.1290],\n",
       "         [ 1.0000, -0.0300, -0.1290]],\n",
       "\n",
       "        [[ 0.6000, -0.2400,  0.1260],\n",
       "         [-2.4000,  1.2700, -0.1290]]], size=(2, 2, 3), dtype=torch.qint8,\n",
       "       quantization_scheme=torch.per_channel_affine,\n",
       "       scale=tensor([0.1000, 0.0100, 0.0010], dtype=torch.float64),\n",
       "       zero_point=tensor([-1,  0,  1]), axis=2)"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "torch.quantize_per_channel(float_tensor, axis=2, scales=torch.tensor([1e-1, 1e-2, 1e-3]), zero_points=torch.tensor([-1, 0, 1]), dtype=torch.qint8)"
   ]
  },
  {
   "source": [
    "## `Int` to quantized int (`qint`, value)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([0.2200, 0.5800, 0.3100, 0.3300, 0.4900, 0.9000, 0.2000, 0.9800, 0.2900,\n",
       "        0.1600], size=(10,), dtype=torch.quint8,\n",
       "       quantization_scheme=torch.per_tensor_affine, scale=0.01, zero_point=0)"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "int_tensor = torch.randint(0, 100, size=(10,), dtype=torch.uint8)\n",
    "q = torch._make_per_tensor_quantized_tensor(int_tensor, 1e-2, 0)\n",
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(0.3300, size=(), dtype=torch.quint8,\n       quantization_scheme=torch.per_tensor_affine, scale=0.01, zero_point=0)\ntensor(0.4400, size=(), dtype=torch.quint8,\n       quantization_scheme=torch.per_tensor_affine, scale=0.01, zero_point=0)\n"
     ]
    }
   ],
   "source": [
    "# 수정도 가능\n",
    "\n",
    "print(q[3])\n",
    "q[3] = 0.444\n",
    "print(q[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "q.is_quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<function Tensor.size>"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "q.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([22, 58, 31, 44, 49, 90, 20, 98, 29, 16], dtype=torch.uint8)"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "q.int_repr()"
   ]
  },
  {
   "source": [
    "# Introduction to quantization on pytorch\n",
    "\n",
    "https://pytorch.org/blog/introduction-to-quantization-on-pytorch/\n",
    "https://pytorch.org/docs/stable/quantization.html\n",
    "\n",
    "## 1. Dynamic Quantization\n",
    "\n",
    "이미 학습된 모델 -> quantization이 필요한 부분만 quantization.  \n",
    "저장된 Weight를 memory에 불러오는 것이 bottle-neck인 경우. 일정 Weight를 int로 저장하므로써 가볍게 만든다. (e.g. BERT의 linear weight, [BERT example](https://pytorch.org/tutorials/intermediate/dynamic_quantization_bert_tutorial.html))\n",
    "\n",
    "## 2. Post-Training Static Quantization\n",
    "\n",
    "이미 학습된 모델 -> 처음부터 끝까지 quantization.  \n",
    "전체 모델의 inference 시간이 오래 걸리는 경우. 사용하면 Computational Cost를 아낄 수 있다.  \n",
    "학습때는 float32로 학습하고 추론시간 단축을 위해서 quantization. (`fbgemm` for x86, `qnnpack` for ARM ...)\n",
    "\n",
    "-  fuse_modules : Quantization을 한 경우 \\[conv, bn\\], \\[conv, bn, relu\\]등을 하나의 모듈처럼 계산할 수 있다. (Computational Cost 이득, but 제한되는 option)\n",
    "\n",
    "Model을 준비한 다음 Calibration을 하기 위해서 input 값을 넣어준다. 이를 통해 activation 값들이 어디에서 주로 존재하는지 확인한 후 \\(min, max\\)등을 사용한다.  \n",
    "마지막으로 이렇게 통과하여 activation 값들이 나온 모델을 quantization 한다.\n",
    "\n",
    "## 3. Quantization Aware Training\n",
    "\n",
    "학습 단계에서 quantization을 할 것을 알고 있는 상황. (정확도도 이득을 보려고 함.)  \n",
    "중간 layer에서 `fq`\\(fake quantization\\)을 넣어줘서 어디부터 어디까지 quantization 하려고 하는지만 알려준다. _\\(따로 추가적인 layer나 int로 학습하거나 하지는 않음.\\)_  \n",
    "=> 따로 calibration이 필요 없음."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}