{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "ps_36",
   "display_name": "ps 3.6 Kernel",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Computational Graph"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Requires_grad, Leaf"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:0\")\n",
    "torch.cuda.set_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User가 만든 ones, zeros, tensor 등은 requires_grad가 default로 False\n",
    "# Gradient를 저장할 것이라는 뜻\n",
    "a = torch.scalar_tensor(2, device=device)\n",
    "b = torch.scalar_tensor(3, device=device)\n",
    "c = torch.scalar_tensor(5, device=device, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1 = a * b\r\n",
    "out2 = out1 + c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "out2.backward()"
   ]
  },
  {
   "source": [
    "메모리 절약을 위해 grad는 최종 leaf에만 저장한다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "False\nFalse\nTrue\nFalse\nTrue\n"
     ]
    }
   ],
   "source": [
    "print(a.requires_grad)\n",
    "print(b.requires_grad)\n",
    "print(c.requires_grad)\n",
    "print(out1.requires_grad)\n",
    "print(out2.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True\nTrue\nTrue\nTrue\nFalse\n"
     ]
    }
   ],
   "source": [
    "print(a.is_leaf)\n",
    "print(b.is_leaf)\n",
    "print(c.is_leaf)\n",
    "print(out1.is_leaf)\n",
    "print(out2.is_leaf)"
   ]
  },
  {
   "source": [
    "## requires_grad 속성의 전이\n",
    "\n",
    "a, b가 requires_grad가 꺼져 있다면 가장 마지막으로 requires_grad가 True인 곳이 leaf가 된다.  \n",
    "위의 경우 a, b에는 requires_grad 저장 X => out1도 leaf가 된다. 하지만 requires_grad는 False."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.scalar_tensor(2, device=device, requires_grad=True)\n",
    "b = torch.scalar_tensor(3, device=device, requires_grad=True)\n",
    "c = torch.scalar_tensor(5, device=device, requires_grad=True)\n",
    "out1 = a * b\n",
    "out2 = out1 + c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True\nTrue\nTrue\nTrue\nTrue\n"
     ]
    }
   ],
   "source": [
    "print(a.requires_grad)\n",
    "print(b.requires_grad)\n",
    "print(c.requires_grad)\n",
    "print(out1.requires_grad)\n",
    "print(out2.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True\nTrue\nTrue\nFalse\nFalse\n"
     ]
    }
   ],
   "source": [
    "print(a.is_leaf)\n",
    "print(b.is_leaf)\n",
    "print(c.is_leaf)\n",
    "print(out1.is_leaf)\n",
    "print(out2.is_leaf)"
   ]
  },
  {
   "source": [
    "# Checking Grad\n",
    "\n",
    "## Retain_grad"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "out2.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(3., device='cuda:0')\ntensor(2., device='cuda:0')\ntensor(1., device='cuda:0')\nNone\nNone\n"
     ]
    }
   ],
   "source": [
    "print(a.grad)\n",
    "print(b.grad)\n",
    "print(c.grad)\n",
    "\n",
    "# 기본적으로는 grad를 저장하지 않음 (leaf가 아니므로)\n",
    "print(out1.grad)\n",
    "print(out2.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1.retain_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(3., device='cuda:0')\ntensor(2., device='cuda:0')\ntensor(2., device='cuda:0')\ntensor(1., device='cuda:0')\nNone\n"
     ]
    }
   ],
   "source": [
    "print(a.grad)\n",
    "print(b.grad)\n",
    "print(c.grad)\n",
    "\n",
    "print(out1.grad)\n",
    "print(out2.grad)"
   ]
  },
  {
   "source": [
    "## Hooking\n",
    "\n",
    "Backward를 하면서 grad를 사용하고 원래는 버림(deallocation).  \n",
    "Hooking = 버리기 전에 잠시 가져오는 것"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hooking(grad):\n",
    "    print(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.scalar_tensor(2, device=device, requires_grad=True)\n",
    "b = torch.scalar_tensor(3, device=device, requires_grad=True)\n",
    "c = torch.scalar_tensor(5, device=device, requires_grad=True)\n",
    "out1 = a * b\n",
    "out2 = out1 + c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x1d01c5c50f0>"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "out1.register_hook(hooking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(1., device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "out2.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1.grad \n",
    "# Hook하고 버렸기 때문에\n",
    "# 여전히 None"
   ]
  },
  {
   "source": [
    "# Backward는 어떻게 계산되는가?\n",
    "\n",
    "## Linear Function ([Document](https://pytorch.org/docs/stable/notes/extending.html))\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Grad를 계산하면서 graph의 선을 끊고 garbage collection을 진행한다. 메모리 이득."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.scalar_tensor(2, device=device, requires_grad=True)\n",
    "b = torch.scalar_tensor(3, device=device, requires_grad=True)\n",
    "c = torch.scalar_tensor(5, device=device, requires_grad=True)\n",
    "out1 = a * b\n",
    "out2 = out1 + c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "out2.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time.",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-f92619d17b0b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mout2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\anaconda3\\envs\\ps_36\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda3\\envs\\ps_36\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "# 그래프가 부서져서 두 번 backward 불가능\n",
    "# out2.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.scalar_tensor(2, device=device, requires_grad=True)\n",
    "b = torch.scalar_tensor(3, device=device, requires_grad=True)\n",
    "c = torch.scalar_tensor(5, device=device, requires_grad=True)\n",
    "out1 = a * b\n",
    "out2 = out1 + c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "out2.backward(retain_graph=True)\n",
    "out2.backward(retain_graph=True)\n",
    "out2.backward(retain_graph=True) # 여러 번 실행 가능"
   ]
  },
  {
   "source": [
    "### Optimizer zero grad ([Document](https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html#use-parameter-grad-none-instead-of-model-zero-grad-or-optimizer-zero-grad))\n",
    "\n",
    "`optimizer.zero_grad()` 보다 `optimizer.zero_grad(set_to_none=True)`가 빠르다고 함. (pytorch >= 1.7)  \n",
    "0으로 바꾸는 것은 메모리를 비우는 것이 아니라 `None`으로 바꿔 주어야 한다. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}