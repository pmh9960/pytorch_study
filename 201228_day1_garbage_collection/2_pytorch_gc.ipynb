{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "ps_36",
   "display_name": "ps 3.6 Kernel",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Pytorch에서 Garbage Collection"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:0\")\n",
    "torch.cuda.set_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torch.empty((1,), device=device) # empty 1개임에도 메모리는 800~ MB 차지. => torch의 kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "del test"
   ]
  },
  {
   "source": [
    "## Cached Memory\n",
    "\n",
    "메모리를 매 번 초기화 하는 것보다 자리를 먼저 잡아두고 할당하는 방식이 빠르고 좋다.  \n",
    "그렇기 때문에 모델을 돌릴 때 forward / backward propagation 한 번 씩 돌리고 시작하는 것도 좋은 방법."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torch.randn((1000,1000,300), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "del test # 여전히 메모리 차지하고 있음 (cached memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache() # Cached memory를 비워줌 / 하지만 많은 실험들에서 일일이 이 작업을 하지는 않음"
   ]
  },
  {
   "source": [
    "# 그 대신 중요한 명령어 두 개"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = torch.randn((1000,1000,300), device=device)\n",
    "test2 = test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1200000000"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "torch.cuda.memory_allocated() # 할당된 메모리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1203765248"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "torch.cuda.memory_reserved() # 할당된 메모리 (Cache 포함)"
   ]
  },
  {
   "source": [
    "### 기타 명령어..."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'device': 0,\n",
       "  'address': 21495808000,\n",
       "  'total_size': 2097152,\n",
       "  'allocated_size': 0,\n",
       "  'active_size': 0,\n",
       "  'segment_type': 'small',\n",
       "  'blocks': [{'size': 2097152, 'state': 'inactive'}]},\n",
       " {'device': 0,\n",
       "  'address': 22749904896,\n",
       "  'total_size': 1201668096,\n",
       "  'allocated_size': 1200000000,\n",
       "  'active_size': 1200000000,\n",
       "  'segment_type': 'large',\n",
       "  'blocks': [{'size': 1200000000, 'state': 'active_allocated'},\n",
       "   {'size': 1668096, 'state': 'inactive'}]}]"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "torch.cuda.memory_snapshot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1200006144\n1203765248\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.max_memory_allocated()) # 현재까지 최대 값을 출력\r\n",
    "print(torch.cuda.max_memory_reserved())\r\n",
    "\r\n",
    "# torch.cuda.reset_max_memory_allocated() # 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1203765248"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}